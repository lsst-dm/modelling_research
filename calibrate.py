# This file is part of modelling_research.
#
# Developed for the LSST Data Management System.
# This product includes software developed by the LSST Project
# (https://www.lsst.org).
# See the COPYRIGHT file at the top-level directory of this distribution
# for details of code ownership.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

from . import dc2
from lsst.afw.table import Field, Schema, SchemaMapper, SourceCatalog
import lsst.daf.butler as dafButler
from . import meas_model as mm
import os
from . import tables
import time
from .timing import time_print
from timeit import default_timer as timer


def calibrate_catalog(catalog, photoCalibs_filter, filter_ref=None, func_field=None):
    """Calibrate a catalog with filter-dependent instFlux columns.

    Parameters
    ----------
    catalog : `lsst.afw.table.SourceCatalog`
        A SourceCatalog with instFlux columns.
    photoCalibs_filter : `dict` [`str`, `lsst.afw.image.PhotoCalib`]
        A dict of PhotoCalibs by filter.
    filter_ref : `str`
        The filter to use for non-multiband fluxes; default first key in list(photoCalibs_filter).
    func_field : callable
        A function that takes a field name and returns true if it should be calibrated. Defaults

    Returns
    -------
    catalog : `lsst.afw.table.SourceCatalog`
        A SourceCatalog with instFlux columns calibrated to mag columns.

    Notes
    -------
    The main purpose of this method is to calibrate a catalog with multi-band MultiProFit columns as
    generated by this task; however, it will work on any catalog as long as the columns match one of
    [*fit_*instFlux, *Flux_instFlux], and the filter-dependent columns are named *{filter}_instFlux.

    Only fields ending in '_instFlux' are calibrated; this is in addition to passing the `func_field` check.
    """
    if func_field is None:
        func_field = mm.is_field_fit
    filters = photoCalibs_filter.keys()
    if filter_ref is None:
        filter_ref = list(filters)[0]
    # See DM-23766: catalog.schema.getNames() returns a set (unordered) whereas we want a list
    columns = [x.field.getName() for x in catalog.schema]
    fluxes = ['_'.join(name.split('_')[0:-1]) for name in columns if
              (func_field(name) and name.endswith('_instFlux'))
              or name.endswith('Flux_instFlux')
              or name.endswith('SdssShape_instFlux')]
    fluxes_filter = {band: [] for band in filters}
    for flux in fluxes:
        last = flux.split('_')[-1]
        fluxes_filter[last if last in filters else filter_ref].append(flux)
    for band, photoCalib in photoCalibs_filter.items():
        catalog = photoCalib.calibrateCatalog(catalog, fluxes_filter[band])
    return catalog


def calibrate_catalogs(
        files, butler, func_dataId=None, is_dc2=False, use_butler=True, return_cats=False, write=True,
        files_ngmix=None, datasetType=None, datasetType_ngmix=None, butler_scarlet=None,
        datasetType_scarlet=None, fields_scarlet=None, postfix='_mag.fits', type_cat=None, type_calib=None,
        get_cmodel_forced=False, overwrite_band=None, log=True,
        n_retry_max=0, retry_delay=0, skip_newer=True, kwargs_get=None,
        **kwargs
):
    """Calibrate and concatenate source catalogs.

    Parameters
    ----------
    files : iterable of `str`
        An iterable of paths to catalogs to calibrate.
    butler : `lsst.daf.persistence.Butler` or container thereof
        A `dict` of butlers if `dc2`, else a single butler to obtain photoCalibs from.
    func_dataId : callable
        A function that takes a catalog filename sans FITS extension and returns bands, patch, tract.
    is_dc2 : `bool`
        Whether this is a DC2 simulation repo on lsst-dev.
    use_butler : `bool`
        Whether to retrieve catalogs from the butler rather than reading from disk.
    return_cats : `bool`
        Whether to return the calibrate catalogs; they might be very large so the default is False.
    write : `bool`
        Whether to write the calibrated catalogs to disk, postfixed by `postfix.
    files_ngmix : iterable of `str` or `lsst.daf.persistence.Butler`
        An iterable of paths to ngmix catalogs to calibrate (must be same len as `files`) or a Butler thereof.
    datasetType_ngmix : `str`
        The butler dataset type to retrieve for ngmix files.
    butler_scarlet : `lsst.daf.persistence.Butler`
        A butler containing scarlet outputs.
    fields_scarlet : iterable of `str`
        List of scarlet field names to copy from `butler_scarlet`.
    datasetType_scarlet : `str`
        The butler dataset type to retrieve for scarlet columns; default 'deepCoadd_deblendedModel'.
    postfix : `str`
        The postfix to add to filenames when writing calibrate catalogs; default '_mag.fits'.
    type_cat: type
        A type of catalog to read; default `lsst.afw.table.SourceCatalog`.
    type_calib : `str`
        The type of calibration to use; default "deepCoadd_photoCalib".
    get_cmodel_forced: `bool`
        Whether to add cmodel forced photometry columns.
    overwrite_band: `str` or None
        Filter to load measurement catalog to overwrite pre-existing (non-MultiProFit) fields with
        (e.g. if MultiProFit used the reference "_ref" SourceCatalog but should contain "_meas" output).
    log : `bool`
        Whether to print progress updates.
    n_retry_max : `int`
        Number of times to retry failed catalog reads.
    retry_delay : `float`
        Delay time in seconds before retrying failed catalog reads. Ignored if not greater than zero.
    skip_newer : `bool`
        Whether to skip calibrating catalogs where the output file exists and is newer than the input.
    **kwargs
        Additional arguments to pass to `calibrate_catalog`.

    Returns
    -------
    cats : `list` [`lsst.afw.table.SourceCatalog`]
        Calibrate catalogs for each file if `return_cat`.

    Notes
    -----
    CModel forced photometry has a fixed shape in each band but different amplitudes.

    Retry arguments are useful for calibrating catalogs written by tasks that are still running, since they
    writing may still be in progress when they're set to be read.
    """
    if retry_delay is None or not (retry_delay > 0):
        retry_delay = 0
    if func_dataId is None:
        func_dataId = parse_multiprofit_dataId
    if is_dc2:
        tracts_dc2 = dc2.get_tracts()
        repos = dc2.get_repos()
        if butler is None:
            butler = {}
    if type_calib is None:
        type_calib = 'deepCoadd.photoCalib'
    if datasetType is None:
        datasetType = 'deepCoadd_multiprofit'
    if kwargs_get is None:
        kwargs_get = {}

    if files_ngmix is not None:
        is_ngmix_butler = isinstance(files_ngmix, dafButler.Butler)
        if is_ngmix_butler:
            if datasetType_ngmix is None:
                datasetType_ngmix = 'deepCoadd_ngmix_deblended'
        elif not (len(files_ngmix) == len(files)):
            raise RuntimeError(f'len(files_ngmix)={len(files_ngmix)} != len(files)={len(files)}')
    else:
        is_ngmix_butler = False
    if type_cat is None:
        type_cat = SourceCatalog
    if butler_scarlet is not None:
        if datasetType_scarlet is None:
            datasetType_scarlet = 'deepCoadd_deblendedFlux'
        if fields_scarlet is None:
            fields_scarlet = ["deblend_edgePixels", "deblend_scarletFlux"]

    time_init = timer()
    time_now = time_init
    n_files = len(files)
    cats = []

    for idx, file in enumerate(files):
        filename = file.split('.fits')[0]
        filename_out = f'{filename}{postfix}'
        if skip_newer and all(os.path.isfile(fname) for fname in (file, filename_out)) and (
                os.path.getmtime(filename_out) > os.path.getmtime(file)):
            print(f'Skipping {file} because output file {filename_out} exists and is newer')
        else:
            bands, tract, patch = func_dataId(filename)
            dataId = {'tract': tract, 'patch': patch}

            if is_dc2:
                version = tracts_dc2[tract][1]
                if version not in butler:
                    path = repos[version]
                    print(f'DC2 butler {path} for {tract} not found; loading...')
                    butler[version] = dafButler.Butler(path)
                    # Ignore the time spent reading butlers for the ETA
                    time_init -= time_now
                    time_print(time_now, prefix=f'Loaded butler in ')
                    time_init += time_now
                butler_cal = butler[version]
            else:
                butler_cal = butler

            if log:
                preprint = "Unknown" if (idx == 0) else f'{(time_now - time_init) * (n_files - idx) / idx:.1f}s'
                print(f'ETA={preprint}; Calibrating {file}... ', end='')
            if use_butler:
                cat = butler_cal.get(
                    datasetType,
                    **(set_dataId_band(dataId, bands[0]) if (datasetType == "deepCoadd_meas") else dataId),
                    **kwargs_get,
                )
            else:
                for retry in range(1 + n_retry_max):
                    try:
                        cat = type_cat.readFits(file)
                    except:
                        if retry == n_retry_max:
                            raise
                        elif retry_delay:
                            time.sleep(retry_delay)
            if log:
                time_now = time_print(time_now, prefix='Read in ')

            if files_ngmix or get_cmodel_forced or (butler_scarlet is not None):
                mapper = SchemaMapper(cat.schema)
                mapper.addMinimalSchema(cat.schema, True)
                mapper.editOutputSchema().setAliasMap(cat.schema.getAliasMap())

            fields_cat_new = []

            if files_ngmix is not None:
                cat_ngmix = files_ngmix.get(datasetType_ngmix, dataId, **kwargs_get) if \
                    is_ngmix_butler else type_cat.readFits(files_ngmix[idx])
                schema_ngmix = cat_ngmix.schema
                names = cat.schema.getNames()
                fields_new = {}
                for key in schema_ngmix:
                    field = key.field
                    name = field.getName()
                    if name not in names:
                        name_split = name.split('_')
                        n_split = len(name_split)
                        is_flux = name_split[-2] == "flux"
                        is_flux_err = (n_split > 2) and (name_split[-3] == 'flux') and (name_split[-2] == 'err')
                        is_psf = name_split[1] == 'psf'
                        if is_flux:
                            band = name_split[-1]
                            name_new = f'{"_".join(name_split[:-2])}_{band}_instFlux' if band in bands else None
                        elif is_flux_err:
                            band = name_split[-1]
                            name_new = f'{"_".join(name_split[:-3])}_{band}_instFluxErr'\
                                if band in bands else None
                        elif is_psf and not (
                                ((n_split > 3) and (name_split[3] == 'mean')) or (
                                (n_split > 2) and (name_split[2] == 'flags'))):
                            band = name_split[2]
                            name_new = name if band in bands else None
                        else:
                            name_new = name
                        if name_new is not None:
                            fields_new[name] = name_new
                            mapper.editOutputSchema().addField(field.copyRenamed(name_new))
                fields_cat_new.append((fields_new, cat_ngmix))

            if butler_scarlet is not None:
                cats_scarlet = {}
                for band in bands:
                    cat_scarlet = butler_scarlet.get(datasetType_scarlet, **set_dataId_band(dataId, band), **kwargs_get)
                    cats_scarlet[band] = cat_scarlet
                    schema_scarlet = cat_scarlet.schema
                    fields_new = {}
                    for field in fields_scarlet:
                        is_flux = field == "deblend_scarletFlux"
                        name_new = f'scarlet_{band}_{field if not is_flux else "instFlux"}'
                        fields_new[field] = name_new
                        key = schema_scarlet.find(field)
                        if is_flux:
                            mapper.editOutputSchema().addField(Field(dtype='D', name=name_new, doc=key.field.getDoc()))
                        else:
                            mapper.editOutputSchema().addField(key.field.copyRenamed(name_new))
                    fields_cat_new.append((fields_new, cat_scarlet))

            if get_cmodel_forced:
                schema = None
                fields_out = []
                for band in bands:
                    forced = butler_cal.get('deepCoadd_forced_src', set_dataId_band(dataId, band), **kwargs_get)

                    if schema is None:
                        schema = forced.schema
                        for key in schema:
                            field = key.field
                            name_field = field.getName()
                            if mm.is_field_modelfit(name_field):
                                name_split = name_field.split('_')[1:]
                                fields_out.append((
                                    name_field,
                                    f'modelfit_forced_{"_".join(name_split[:-1])}_',
                                    f'_{name_split[-1]}'
                                ))
                    elif forced.schema != schema:
                        raise RuntimeError(f'Schema {forced.schema} for dataId {dataId} deepCoadd_forced_src '
                                           f'differs from filter {bands[0]} schema {schema}')

                    fields_new = {}
                    for field_in, prefix_field, postfix_field in fields_out:
                        name_new = f'{prefix_field}{band}{postfix_field}'
                        fields_new[field_in] = name_new
                        mapper.editOutputSchema().addField(
                            forced.schema.find(field_in).field.copyRenamed(name_new))
                    fields_cat_new.append((fields_new, forced))

            if fields_cat_new:
                cat_new = SourceCatalog(mapper.getOutputSchema())
                cat_new.reserve(len(cat))
                cat_new.extend(cat, mapper)
                for fields_new, cat_in in fields_cat_new:
                    for field_in, field_out in fields_new.items():
                        cat_new[field_out] = cat_in[field_in]
                cat = cat_new

            n_columns = len(cat.schema.getNames())
            if n_columns > tables.n_columns_max:
                raise RuntimeError(f'pre-calib cat has {n_columns}>max={tables.n_columns_max}')

            try:
                photoCalibs = {band: butler_cal.get(type_calib, set_dataId_band(dataId, band), **kwargs_get)
                               for band in bands}
            except:
                print(f'Failed generating photoCalibs of type={type_calib} with dataId={dataId}')
                raise

            if overwrite_band is not None:
                meas = butler_cal.get('deepCoadd_meas', set_dataId_band(dataId, overwrite_band), **kwargs_get)
                for key in meas.schema:
                    field = key.field
                    # It's probably better to keep the original flag that MultiProFit used
                    if field.dtype != 'Flag' and field.dtype != 'String':
                        name_field = field.getName()
                        cat[name_field] = meas[name_field]

            cat_calib = calibrate_catalog(cat, photoCalibs, **kwargs)
            if return_cats:
                cats.append(cat_calib)
            if write:
                n_columns = len(cat_calib.schema.getNames())
                if n_columns > tables.n_columns_max:
                    tables.write_split_cat_fits(filename_out, cat, cat_calib)
                else:
                    cat_calib.writeFits(filename_out)
                if log:
                    info = f'{filename_out} ' if write else ''
                    time_now = time_print(time_now, prefix=f"Calibrated {info}in ")
    if return_cats:
        return cats


def parse_multiprofit_dataId(filename):
    bands, tract, patch = filename.split('/')[-1].split('_')[-3:]
    return bands, int(tract), int(patch) if patch.isdigit() else patch


def parse_multiprofit_dataId_Hsc(filename):
    bands, tract, patch = parse_multiprofit_dataId(filename)
    bands = tuple(f'HSC-{b.upper()}' for b in bands)
    return bands, int(tract), int(patch) if patch.isdigit() else patch


def reorder_fields(cat, filters=None, func_field=None):
    """Reorder filter-dependent instFlux fields in a catalog.

    Parameters
    ----------
    cat : `lsst.afw.table.Catalog`
        A catalog with fields to be re-ordered.
    filters : iterable of `str`
        The filters in their desired order. Default ('i', 'r', 'g').
    func_field : callable
        A function that takes a string field name and returns whether it should be re-ordered. Default
        `modelling_research.meas_model.is_field_multiprofit`.

    Returns
    -------
    A catalog with re-ordered fields.

    Notes
    -----
    If no re-ordering is necessary, the function will return cat. Otherwise, it will make a deep copy of cat
    before re-assigning the out-of-order columns. This may not be the optimal.
    """
    if filters is None:
        filters = ('i', 'r', 'g')
    if func_field is None:
        func_field = mm.is_field_multiprofit
    filters_order = {idx: band for idx, band in enumerate(filters)}

    schema = cat.schema
    schema_new = Schema()
    fields = [x.field.getName() for x in cat.schema]
    fields_added = {}
    fields_remap = set()

    for field in fields:
        field_toadd = field
        if func_field(field) and field.endswith('_instFlux'):
            split = field.split('_')
            name_field = '_'.join(split[:-2])
            band = split[-2]
            if name_field not in fields_added:
                fields_added[name_field] = 0
            else:
                fields_added[name_field] += 1

            band_ordered = filters_order[fields_added[name_field]]

            if band != band_ordered:
                field_ordered = f'{name_field}_{band_ordered}_{split[-1]}'
                fields_remap.add(field)
                field_toadd = field_ordered
        schema_new.addField(schema.find(field_toadd).field)

    if not fields_remap:
        return cat

    schema_new.setAliasMap(schema.getAliasMap())
    cat_new = type(cat)(schema_new)
    cat_new.extend(cat, deep=True)

    for field in fields_remap:
        cat_new[field] = cat[field]

    return cat_new


def set_dataId_band(dataId, band):
    dataId['band'] = band
    return dataId
